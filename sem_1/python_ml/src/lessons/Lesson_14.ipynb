{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Факультет \"Прикладная математика\" МАИ</h1>\n",
    "<h2>Курс \"Основы Python для анализа данных\"</h2>\n",
    "<h2>Артамонов Игорь Михайлович</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Занятие № 14. Машинное обучение на текстовых данных.</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общение / вопросы по курсу\n",
    "\n",
    "Платформа для групповой работы Atlassian Confluence факультета \"Прикладная математика\"\n",
    "\n",
    "https://mai.moscow/display/PYTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>Занятие № 14. Машинное обучение на текстовых данных</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## virtualenv + Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<Ctrl> + <Alt> + T - новое окно терминала\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ conda -V\n",
    "\n",
    "$ conda update conda\n",
    "\n",
    "$ conda search \"^python$\"\n",
    "\n",
    "$ conda create -n yourenvname python=x.x anaconda\n",
    "\n",
    "$ source activate yourenvname\n",
    "\n",
    "$ jupyter notebook\n",
    "\n",
    "$ conda install -n yourenvname [package]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>К прошлой лекции:</b>\n",
    "    \n",
    "\n",
    "There are a few discussions for <b>Epoch Vs Iteration</b>.\n",
    "\n",
    "Iteration is one time processing for forward and backward for a batch of images (say one batch is defined as 16, then 16 images are processed in one iteration).\n",
    "\n",
    "Epoch is once all images are processed one time individually of forward and backward to the network, then that is one epoch.\n",
    "\n",
    "I like to make sure my definition of epoch is correct.\n",
    "\n",
    "One epoch is counted when (Number of iterations * batch size) / total number of images in training\n",
    "\n",
    "One epoch is counted when Number of iterations == total number of images in training\n",
    "\n",
    "Which one is correct epoch? My selection is the first one.\n",
    "\n",
    "---\n",
    "One iteration means one batch processed. One epoch means all data processed one times.\n",
    "\n",
    "So one epoch is counted when (batch_size * number_iteration) >= number_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">ВОПРОС</a>\n",
    "Какие задачи машинного обучения для __текстов__ могут быть:\n",
    "* регрессией?\n",
    "* классификацией?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Обработка текстов</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### О чем говорим\n",
    "* стандартные возможности (их много!)\n",
    "* частотный подход к тексту:\n",
    "    - \"мешок слов\" (bag of words), OHE\n",
    "    - CV\n",
    "    - TF/IDF\n",
    "* \"глубокий\" разбор текста (не путать с глубокими / глубинными сетями!)\n",
    "* рекуррентные нейронные сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sc\n",
    "from numpy.random import randn\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib.patches import Ellipse\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Текст \n",
    "(как мы его воспринимаем) - это набор:\n",
    "* слов\n",
    "* знаков препинания\n",
    "* нетекстовой информации (например, вставленных изображений или формул) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">ВОПРОС</font>\n",
    "Можно ли отнести таблицу к тексту?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что может быть важно:\n",
    "* какие слова?\n",
    "* последовательность слов\n",
    "* частота слов\n",
    "* свойства слов\n",
    "* последовательность предложений\n",
    "* текст в целом\n",
    "* свойства текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузим датасет с 100,000 обзоров курсов с kaggle:<br>\n",
    "<a href=\"https://www.kaggle.com/septa97/100k-courseras-course-reviews-dataset\">100K Coursera's Course Reviews Dataset</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data14/100k-courseras-course-reviews-dataset/reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'Review', 'Label'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    79173\n",
       "4    18054\n",
       "3     5071\n",
       "1     2469\n",
       "2     2251\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Также будем работать отзывами с imdb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 10000\n",
    "\n",
    "print('Loading data...')\n",
    "# (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "# print(len(x_train), 'train sequences')\n",
    "# print(len(x_test), 'test sequences')\n",
    "\n",
    "# save np.load\n",
    "np_load_old = np.load\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "# call load_data with allow_pickle implicitly set to true\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "# restore np.load for future normal usage\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,   14,   22,   16,   43,  530,  973, 1622, 1385,   65,  458,\n",
       "       4468,   66, 3941,    4,  173,   36,  256,    5,   25,  100,   43,\n",
       "        838,  112,   50,  670,    2,    9,   35,  480,  284,    5,  150,\n",
       "          4,  172,  112,  167,    2,  336,  385,   39,    4,  172, 4536,\n",
       "       1111,   17,  546,   38,   13,  447,    4,  192,   50,   16,    6,\n",
       "        147, 2025,   19,   14,   22,    4, 1920, 4613,  469,    4,   22,\n",
       "         71,   87,   12,   16,   43,  530,   38,   76,   15,   13, 1247,\n",
       "          4,   22,   17,  515,   17,   12,   16,  626,   18,    2,    5,\n",
       "         62,  386,   12,    8,  316,    8,  106,    5,    4, 2223, 5244,\n",
       "         16,  480,   66, 3785,   33,    4,  130,   12,   16,   38,  619,\n",
       "          5,   25,  124,   51,   36,  135,   48,   25, 1415,   33,    6,\n",
       "         22,   12,  215,   28,   77,   52,    5,   14,  407,   16,   82,\n",
       "          2,    8,    4,  107,  117, 5952,   15,  256,    4,    2,    7,\n",
       "       3766,    5,  723,   36,   71,   43,  530,  476,   26,  400,  317,\n",
       "         46,    7,    4,    2, 1029,   13,  104,   88,    4,  381,   15,\n",
       "        297,   98,   32, 2071,   56,   26,  141,    6,  194, 7486,   18,\n",
       "          4,  226,   22,   21,  134,  476,   26,  480,    5,  144,   30,\n",
       "       5535,   18,   51,   36,   28,  224,   92,   25,  104,    4,  226,\n",
       "         65,   16,   38, 1334,   88,   12,   16,  283,    5,   16, 4472,\n",
       "        113,  103,   32,   15,   16, 5345,   19,  178,   32])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">ВОПРОС</font>\n",
    "Это точно текст?\n",
    "Если да, то что это всё значит?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общие положения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Порядок__ обработки текстов зависит от __цели__ обработки!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Возможные операции:__\n",
    "* удаляем всю (или часть) содержания, не являющееся текстом (картинки, знаки препинания и т.д.)\n",
    "* переводим в один (обычно - нижний) регистр\n",
    "* извлекаем информацию с помощью regexp\n",
    "* нормализуем слова (существительные - единственно число, мужской род, глаголы - в неопределенную форму и т.д)\n",
    "* делаем встраивание:\n",
    "    - частотное\n",
    "    - векторное\n",
    "* извлекаем информацию из полученного представления текста\n",
    "* используем полученнц информацию "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nltk.org/book/\">NLTK Book</href> - https://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стандартные средства работы с текстом в Python\n",
    "* текст - это не только тексты!\n",
    "* string - стандартные средства работы со строками\n",
    "* re - регулярные выражения\n",
    "* unicodedata - работа с символами UNICODE (уже менее актуально, но ...)\n",
    "* специфические парсеры:\n",
    "    - JSON\n",
    "    - URL (urllib)\n",
    "    - HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Встраивание\" слов (word embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                      good and interesting\n",
       "1         This class is very helpful to me. Currently, I...\n",
       "2         like!Prof and TAs are helpful and the discussi...\n",
       "3         Easy to follow and includes a lot basic and im...\n",
       "4         Really nice teacher!I could got the point eazl...\n",
       "                                ...                        \n",
       "107013    Trendy topic with talks from expertises in the...\n",
       "107014    Wonderful! Simple and clear language, good ins...\n",
       "107015     an interesting and fun course. thanks. dr quincy\n",
       "107016    very broad perspective, up to date information...\n",
       "107017    An informative course on the social and financ...\n",
       "Name: Review, Length: 107018, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=['good and interesting'\n \"This class is very helpful to me. Currently, I'm still learning this class which makes up a lot of basic music knowledge.\"\n 'like!Prof and TAs are helpful and the discussion among students are quite active. Very rewarding learning experience!'\n ... 'an interesting and fun course. thanks. dr quincy'\n 'very broad perspective, up to date information, useful links and videos and good lecturers in general. Thank you for the insights and knowledge.'\n 'An informative course on the social and financial implications due to Zika as well as the factors leading to an epidemic of Zika virus,,'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-99917f4f9c36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m## Преобразуйте Coursera dataset с использованием OneHotEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mrev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReview\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mrev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mrev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    629\u001b[0m                 self._categorical_features, copy=True)\n\u001b[0;32m    630\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 631\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_legacy_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_idx_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_drop_idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, handle_unknown)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mX_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_categories\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'auto'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36m_check_X\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'iloc'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ndim'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;31m# if not a dataframe, do normal check_array validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mX_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m             if (not hasattr(X, 'dtype')\n\u001b[0;32m     51\u001b[0m                     and np.issubdtype(X_temp.dtype, np.str_)):\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    519\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=['good and interesting'\n \"This class is very helpful to me. Currently, I'm still learning this class which makes up a lot of basic music knowledge.\"\n 'like!Prof and TAs are helpful and the discussion among students are quite active. Very rewarding learning experience!'\n ... 'an interesting and fun course. thanks. dr quincy'\n 'very broad perspective, up to date information, useful links and videos and good lecturers in general. Thank you for the insights and knowledge.'\n 'An informative course on the social and financial implications due to Zika as well as the factors leading to an epidemic of Zika virus,,'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "## Преобразуйте Coursera dataset с использованием OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "rev = encoder.fit_transform(df.Review)\n",
    "rev\n",
    "rev.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Слово - в число (обычно - целое)\n",
    "* уменьшаем объем данных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "## Преобразуйте Coursera dataset с использованием LabelEncoder\n",
    "# Ваш код\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Слово - в вектор с одной единицей в определенной позицией\n",
    "\n",
    "Позиционное представление (one-hot encoding (OHE)):\n",
    "    * одно слово - один столбец\n",
    "    * если слово есть - то в столбце 1, иначе - 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "## Преобразуйте Coursera dataset с использованием OneHotEncoder\n",
    "# Ваш код\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">ВОПРОС</font>\n",
    "* Какой тип выходных данных?\n",
    "* Почему?\n",
    "* Что Вы о нём знаете / можете сказать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">ПРОБЛЕМА:</blue>\n",
    "* OHE не обладает смантической близостью<br><br>\n",
    "Но! __Предложения__ в рамках OHE _могут_ обладать!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images14/ohe-words-sentense.png\"><br><br>\n",
    "https://medium.freecodecamp.org/text-classification-and-prediction-using-bag-of-words-8aeb1396cded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images14/bag-of-words-matching.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача классификации состоит в том, чтобы относить ранее неизвестные сущности к тому или иному классу. Примером такой задачи может быть постановка диагноза по медицинским анализам. В этом случае есть два класса результатов: положительный (positive) и отрицательный (negative). Тогда на выходе классификатора может наблюдаться четыре различных ситуации:\n",
    "\n",
    "* Если результат классификации положительный, и истинное значение тоже положительное, то речь идет об истинно-положительном значении (true-positive, TP)\n",
    "* Если результат классификации положительный, но истинное значение отрицательное, то речь идет о ложно-положительном значении (false-positive, FP)\n",
    "* Если результат классификации отрицательный, и истинное значение тоже отрицательное, то речь идет об истинно-отрицательном значении (true-negative, TN)\n",
    "* Если результат классификации отрицательный, но истинное значение положительно, то речь идет о ложно-отрицательном значении (false-negative, FN)\n",
    "\n",
    "\n",
    "https://ru.wikipedia.org/wiki/ROC-кривая"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Precision = \\frac{TP}{TP+FP}\n",
    "$$<br><br>\n",
    "$$\n",
    "Recall = \\frac{TP}{TP+FN}\n",
    "$$<br><br>\n",
    "F-мера представляет собой гармоническое среднее между точностью и полнотой. Она стремится к нулю, если точность или полнота стремится к нулю.<br><br>\n",
    "$$\n",
    "F = 2 \\frac{Precision \\times Recall}{Precision + Recall}\n",
    "$$<br><br>\n",
    "Можно изменить вес точнсти и полноты, придав приоритет одной из этих метрик:<br><br>\n",
    "$$\n",
    "F = (\\beta^2 + 1) \\frac{Precision \\times Recall}{Precision + Recall}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC /AUC\n",
    "<img src=\"images14/plot_roc_0021.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Частотная модель (вектор встречаемости)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images14/count_vec.jpeg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* с каждым словом текста связывается частота его наличия в тексте\n",
    "* встречающиеся слова и их количество являются признаком, характеризующим документ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">ЗАДАНИЕ</font>\n",
    "\n",
    "* Используя CountVectorizer и логистическую регрессию предскажите классы \n",
    "* Для разбения выборки на тренировочную и тестовую воспользуйтесь StratifiedKFold\n",
    "* В качестве метрики используйте ROC/AUC\n",
    "\n",
    "### <font color=\"red\">ВАЖНО!</font>\n",
    "Не забывайте при многоклассовой классификации преобразовывать метки выходного набора с использованием OHE (например, pandas.get_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Ваш код \n",
    "# Настройте параметры векторизатора, если считаете это нужным\n",
    "cv_vect = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Прямая частотность (Term Frequency)__<br><br>\n",
    "\n",
    "$$\n",
    "tf_{i,j} = \\frac{n_{i,j}}{\\sum_k{n_{i,j}}}\n",
    "$$\n",
    "__Обратная частотность документов (IDF Inverse Document Frequency)__ <br><br>\n",
    "$$\n",
    "idf(w) = log \\frac{N}{df_t}\n",
    "$$\n",
    "__TF-IDF__<br><br>\n",
    "$$\n",
    "w_{i,j} = tf_{i,j} \\cdot log (\\frac{N}{df_t})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Редкие слова имеют приоритет при классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">ЗАДАНИЕ</font>\n",
    "\n",
    "* Используя TfidfVectorizer и логистическую регрессию предскажите классы \n",
    "* Для разбения выборки на тренировочную и тестовую воспользуйтесь StratifiedKFold\n",
    "* В качестве метрики используйте ROC/AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['g o o d   a n d   i n t e r e s t i n g', \"T h i s   c l a s s   i s   v e r y   h e l p f u l   t o   m e .   C u r r e n t l y ,   I ' m   s t i l l   l e a r n i n g   t h i s   c l a s s   w h i c h   m a k e s   u p   a   l o t   o f   b a s i c   m u s i c   k n o w l e d g e .\", 'l i k e ! P r o f   a n d   T A s   a r e   h e l p f u l   a n d   t h e   d i s c u s s i o n   a m o n g   s t u d e n t s   a r e   q u i t e   a c t i v e .   V e r y   r e w a r d i n g   l e a r n i n g   e x p e r i e n c e !', 'E a s y   t o   f o l l o w   a n d   i n c l u d e s   a   l o t   b a s i c   a n d   i m p o r t a n t   t e c h n i q u e s   t o   u s e   s k e t c h u p .', 'R e a l l y   n i c e   t e a c h e r ! I   c o u l d   g o t   t h e   p o i n t   e a z l i y   b u t   t h e   v', 'G r e a t   c o u r s e   -   I   r e c o m m e n d   i t   f o r   a l l ,   e s p e c i a l l y   I T   a n d   B u s i n e s s   M a n a g e r s !', 'O n e   o f   t h e   m o s t   u s e f u l   c o u r s e   o n   I T   M a n a g e m e n t !', 'I   w a s   d i s a p p o i n t e d   b e c a u s e   t h e   n a m e   i s   m i s l e a d i n g .   T h e   c o u r s e   p r o v i d e s   a   g o o d   i n t r o d u c t i o n   &   o v e r v i e w   o f   t h e   r e s p o n s i b i l i t i e s   o f   t h e   C T O ,   b u t   h a s   v e r y   l i t t l e   s p e c i f i c a l l y   d i g i t a l   c o n t e n t .   I t   d e a l s   w i t h   t w o - s p e e d   I T   i n   a   s i n g l e   s h o r t   l e c t u r e ,   s o   o f   c o u r s e   t h e   t r e a t m e n t   i s   s u p e r f i c i a l .   I t   i s   e a s y   t o   f i n d   m o r e   i n - d e p t h   m a t e r i a l   f r e e l y   a v a i l a b l e ,   o n   t h e   M c K i n s e y   w e b s i t e   f o r   e x a m p l e .', \"S u p e r   c o n t e n t .   I ' l l   d e f i n i t e l y   r e - d o   t h e   c o u r s e\", 'O n e   o f   t h e   e x c e l l e n t   c o u r s e s   a t   C o u r s e r a   f o r   i n f o r m a t i o n   t e c h n o l o g y   b o s s e s   a n d   m a n a g e r s .', \"I s   t h e r e   a n y   r e a s o n   w h y   y o u   s h o u l d   n o t   a p p l y   t h e   c o u r s e   b y   B C G ? ) I t ' s   c o n t e n t   i s   p r e t t y   u n i q u e   a n d   i n c l u d e s   a   h i g h   l e v e l   a n a l y s i s   a n d   a   w i d e   r a n g e   o f   k n o w l e d g e   n e e d e d   t o   c o v e r   a l l   d e t a i l e d   a s p e c t s . B e s t   r e g a r d s , O l e g   S e r o v\", 'E x c e l l e n t   c o u r s e   a n d   t e a c h e r s .   C o n g r a t u l a t i o n s ! !', 'T h i s   i s   a   g o o d   c o u r s e   f o r   a n   C I O   i n   a   n o n   t e c h n i c a l   c o m p a n y .', 'G o o d   c o n t e n t ,   b u t   t h e   c o u r s e   s e t t i n g   d o e s   ( a t   l e a s t   f o r   m e )   n o t   a l l o w   l e a r n   t h e   c o n t e n t   l o n g   t e r m   d u e   t o   m i s s i n g   r e a d i n g   m a t e r i a l .', 'V e r y   s t r u c t u r e d   a p p r o a c h .   T h a n k   y o u   f o r   s h a r i n g   w i t h   m e .', 'P r o g r a m   d e m y s t i f i e s   t h e   e v o l v i n g   w o r l d   o f   C I O s   i n   a   t y p i c a l   g l o b a l   c o r p o r a t i o n . C o v e r a g e   b e i n g   i n t r o d u c t o r y   f a m i l i a r i z e s   p a r t i c i p a n t s   n i c e l y   t h r o u g h   s o m e   o f   t h e   n u a n c e s   &   e m e r g i n g   t r e n d s . W h e n   I   t o o k   t h e   c o u r s e   p a r t i c i p a n t   n u m b e r s   w a s   f e w e r ,   s o   f e e d b a c k   w a s   q u i t e   d e l a y e d .   S o   o n e   s h o u l d   p l a n   t o   b e   p a t i e n t   t o   g e t   t h e i r   p e e r   r e v i e w s . B a s e d   o n   p e e r   r e v i e w s   i   s e n s e d   m u c h   v a r i a t i o n s   i n   t h e i r   e x p e c t a t i o n s . F e w   s e c t i o n s   a r e   i n   F r e n c h   w i t h   S u b t i t l e s . S u b t i t l e   q u a l i t y   r e q u i r e s   a u d i t / r e v i e w .', 'V e r y   r e l e v a n t   a n d   u s e f u l   c o u r s e   d e s i g n e d   f o r   C I O s', 'T h i s   c o u r s e   d o e s   n o t   s a y   a n y t h i n g   a b o u t   d i g i t i z a t i o n   w h i c h   i s   t h e   c o r e   s u b j e c t   o f   t h e   d i g i t a l   w a v e .', \"V i d e o s   t h a t   a r e   p r e s e n t e d   i n   F r e n c h   c o u l d ' v e   b e e n   t r a n s l a t e d   t o   E n g l i s h .\", 'T h e   c o u r s e   c o n t e n t   i s   q u i t e   g o o d ,   t h o u g h   i t   c o u l d   h a v e   b e e n   d e e p e r   i n   s o m e   a r e a s . B u t   i t s   p e e r   r e v i e w   s y s t e m   i s   n o t   w o r k i n g   w e l l ,   i f   a t   a l l . R e g a r d s ,   A n   L e .', \"G r e a t   p i e c e   o f   w o r k ,   I   e s p e c i a l l y   l i k e d   a   f e w   ' l i f e h a c k s '   f o r   t h e   C I O\", 'E x c e l l e n t   c o u r s e ,   f o r   m e   i t   w a s   v e r y   r e w a r d i n g   a n d   t h e   t e r m s   u s e d   a n d   t h e   t o o l s   g i v e n   w e r e   e x c e l l e n t ,   a n d   t o d a y   a n d   I   p u t   i n   u s e   i n   m y   j o b ,   T h a n k   y o u   f o r   i n c u l c a t i n g   k n o w l e d g e   a n d   m o v e   o n', 'E x c e l l e n t . . .   I t   i s   a   r e p r e s e n t a t i o n   o f   o u r   d a y   b y   d a y ,   t h a n k s   f o r   s h a r i n g   y o u r   v i s i o n ,   e x p e r i e n c e   a n d   a d v i c e s . . . R e g a r d s D a n i e l   D a m a s', 'V e r y   i n t e r e s t i n g   a n d   w e l l - d e s i g n e d   M O O C   !', 'E x c e l l e n t   c o u r s e !   T o t a l l y   r e c o m m e n d e d !', \"G r e a t   c o u r s e   i n f o r m a t i o n   a n d   c o n t e n t   !   V e r y   i n t e r e s t i n g   !   U n f o r t u n a t e l y   t h e r e   i s   a   p e e r   g r a d e d   a s s i g n m e n t   w h i c h   3   c l a s s m a t e s   m u s t   g r a d e .   A l t h o u g h   t w o   o f   m y   c l a s s m a t e s   g a v e   m e   a l l   p a s s i n g   g r a d e s   o n e   c l a s s m a t e   g a v e   m e   a l l   0 ' s   a n d   d i d n ' t   r e a l l y   j u s t i f y   w h y ;   w h i c h   m a d e   m e   h a v e   t o   r e   s u b m i t   m y   a s s i g n m e n t .   T h a n k f u l l y   3   o t h e r   a m a z i n g   c l a s s m a t e s   s a w   t h e   q u a l i t y   o f   m y   w o r k   : )\", 'V e r y   i n t e r e s t i n g   c o u r s e .   I   l e a r n e d   a   l o t   a b o u t   t h e   h i s t o r y   o f   A b o r i g i n a l   e d u c a t i o n .', 'G r e a t   c o n t e n t !   L e a r n e d   a   l o t .', \"T h i s   h a s   b e e n   a n   p h e n o m e n a l   c o u r s e   o n   t h e   e d u c a t i o n   a n d   w o r l d v i e w s   o f   F i r s t   N a t i o n s   p e o p l e .   A s   a n   e d u c a t o r   i n   a   F i r s t   N a t i o n ' s   c o m m u n i t y   i n   O n t a r i o ' s   n o r t h ,   t h e   k n o w l e d g e   i n   t h e   l e c t u r e s   h a s   b e e n   p h e n o m e n a l .\", 'I   f o u n d   t h i s   c o u r s e   w a s   v e r y   i n t e r e s t i n g   a n d   a   g r e a t   l e a r n i n g   t o o l   a n d   I   a m   g l a d   I   w o u l d   r e c o m m e n d   t h i s   c o u r s e   t o   p e o p l e   N i : A w e n', 'W e l l - p r e s e n t e d   c o n t e n t s   w i t h   c o m p r e h e n s i v e   i l l u s t r a t i o n s   a n d   e a s y   t o   f o l l o w ,   c o v e r   i n   p r o p e r   o r d e r   e v e r y   a s p e c t   o f   d r a w i n g   a n d   A u t o C A D .   W o u l d   a p p r e c i a t e   i f   t h e   E n g l i s h   s u b t i t l e   i s   a v a i l a b l e   i n   t h e   w h o l e   c o u r s e ,   s o   m o r e   s t u d e n t s   c o u l d   a c c e s s   t o   t h e s e   h a n d y   l e c t u r e s   a n d   e x e r c i s e s .', 'V e r y   B a s i c   c o u r s e   f o r   l e a r n e r s   w i t h o u t   A u t o c a d   e x p e r i e n c e s .', 'G o o d   q u a l i t y .', 'T h a n k s .', 'C o u r s e   i s   c o m p r e h e n s i v e   a n d   d e t a i l e d .   B u t   i s   a   b i t   f a s t   p a c e d   f o r   p e o p l e   n e w   t o   I T   F i n a n c e', 'A   v e r y   g r e a t   c o u r s e   f o r   a l l   I T   p e r s o n', 'I   t h i n k   i t   s h o u l d   b e   a   m o r e   e n g a g e d   c o u r s e   w i t h   p r a c t i c a l   e x a m p l e s', 'E x c e l l e n t   o v e r v i e w   o f   t h e   w a y s   i n   w h i c h   3 D   p r i n t i n g   i s   i n   u s e   a n d   h o w   i t   m a y   b e   u s e d   i n   t h e   f u t u r e .', 'V e r y   w e l l   d o c u m e n t e d   c o u r s e . A s s i g n m e n t s   a r e   d e s i g n e d   s u c h   a   w a y   t h a t   i t   s t i m u l a t e s   y o u   t o   t h i n k   o u t   o f   b o x .', 'V e r y   G o o d   I n f o r m a t i v e   C o u r s e . ! !', 'E x c e l l e n t   c o u r s e .   I   c o m p l e t e l y   e n j o y e d   i t .   T h a n k   y o u   V i s h a l   a n d   t e a m .', 'I t   o p e n e d   u p   m y   u n d e r s t a n d i n g   o f   w h a t   c o u l d   b e   a c c o m p l i s h e d   w i t h   3 D   p r i n t i n g', 'I m p o r t a n t   i n f o r m a t i o n   o n   c o p y   r i g h t s ,   p a t e n t s ,   e t c .', 'P r o g r e s s   i s   i n   a   s t a t e   o f   f l u x ,   s o   o n e   m u s t   r e a c h   o u t   t o   t h e   b e s t   i n   a c a d e m i a   o n   a n y   p a r t i c u l a r   s u b j e c t   t o   g e t   a   r e a l i s t i c   u p d a t e   o f   t e c h n o l o g y .   T h i s   c o u r s e   w i l l   h e l p   y o u   c a t c h   u p   t o   d e s i g n   t h i n k i n g   a n d   d o i n g .   G e t   i t   a n d   d o   i t .', \"A   r e a l l y   c h a l l e n g i n g   a n d   e n r i c h i n g   c o u r s e   a b o u t   t h e   a m a z i n g   w o r l d   o f   3 D   P r i n t i n g .   I n   t h i s   c o u r s e   y o u   w i l l   d i s c o v e r   t h e   u n i m a g i n a b l e   a m o u n t   o f   a p p l i c a t i o n s   t h a t   3 D   P r i n t i n g   o f f e r s   a n d   w i l l   o p e n   y o u r   m i n d   i n   w a y s   y o u   d i d n ' t   t h i n k   p o s s i b l e !\", \"T h i s   i s   a   r e a l l y   g o o d   s u r v e y   c l a s s   i n   t h e   p o t e n t i a l   o f   3 d   p r i n t i n g ,   w i t h   e x a m p l e s   o f   h o w   i n d i v i d u a l s   c a n   e m p o w e r   t h e m s e l v e s   a n d   o t h e r s   w i t h o u t   t h e   o b s t a c l e s   o f   t r a d i t i o n a l   t o o l i n g ,   w i t h   a   r e l a t i v e l y   s h o r t   l e a r n i n g   c u r v e   a n d   l o w   i n v e s t m e n t .   I t   a l s o   d e m o n s t r a t e s   t h e   w a y s   i n   w h i c h ,   n o w   a n d   i n c r e a s i n g l y   i n   t h e   f u t u r e ,   w e ' l l   a l l   b e   c a p a b l e   o f   d e s i g n i n g   t h e   s o l u t i o n s   t o   o u r   o w n   p r o b l e m s   w i t h   i n c r e a s i n g   a g i l i t y ! T h e   o n l y   t h i n g   t h a t   c o u l d   m a k e   i t   b e t t e r   w o u l d   b e   t o   h a v e   m o r e   d e s i g n   c h a l l e n g e s ! T h a n k s !\", 'E x c e l l e n t   c o u r s e   ,   e n j o y e d   l e a r n i n g   : )', 'G r e a t   c o u r s e   a n d   v e r y   i n f o r m a t i v e .', \"I ' l l   s t a r t   b y   s a y i n g   t h a t   t h i s   c o u r s e   g i v e s   a   g o o d   i n s i g h t   i n t o   a   w i d e   v a r i e t y   o f   a p p l i c a t i o n s   o f   3 D   p r i n t i n g .   T h e r e   a r e   l o t s   o f   i n t e r v i e w s   w i t h   p e o p l e   w h o   h a v e   o r   a r e   l e v e r a g i n g   a d d i t i v e   m a n u f a c t u r i n g   i n   o r i g i n a l   a n d   e f f e c t i v e   w a y s .   T h e   l a s t   w e e k   o f   t h e   c o u r s e   a l s o   i n t r o d u c e s   a n   i n t e r e s t i n g   a n d   s e e m i n g l y   e f f e c t i v e   f r a m e w o r k   f o r   d e s i g n   t h i n k i n g . T h e   r e a s o n   t h a t   I ' v e   o n l y   r a t e d   3   s t a r s   h o w e v e r   i s   t h a t   t h e   o n l y   u s e f u l   t h i n g   y o u   l e a r n   i n   t h i s   c o u r s e   i s   t h e   d e s i g n   t h i n k i n g   f r a m e w o r k .   I   t h i n k   t h a t   i t   w o u l d   b e   b e t t e r   i f   t h e s e   v i d e o s   w e r e   s c a t t e r e d   t h r o u g h o u t   t h e   r e s t   o f   t h e   s p e c i a l i s a t i o n .   I   d o   s e e   v a l u e   i n   i n c l u d i n g   t h i s   m a t e r i a l   i n   t h e   s p e c i a l i s a t i o n   b u t   n o t   a s   a n   o u t r i g h t   c o u r s e .\", \"T h i s   c o u r s e   d o e s n ' t   c o n t a i n   a n y   n e w   i n f o r m a t i o n .   I t   d o e s   n o t   t e a c h   y o u   b u t   j u s t   e x c i t e d l y   s h o w s   c o m m o n l y   k n o w n   f a c t s . T h e r e   a r e   b e t t e r   w a y s   t o   i n v e s t   y o u r   t i m e .\", 'G r e a t   c o u r s e .   I   e n j o y e d   l e a r n i n g   t h e   m a t e r i a l .', 'E x c e l l e n t   c o u r s e .   G o o d   c l a s s e s ,   e a s y   t o   f o l l o w ,   u p d a t e d   c o n t e n t ,   h i g h   q u a l i t y   v i d e o s .', 'I t   s h o u l d   b e   m e r g e d   w i t h   c o u r s e   1', 'V e r y   n u t r i t i o n a l   c o u r s e   w i t h   r e a l   w o r l d   e x a m p l e s   t h a t   l e t   y o u   u n d e r s t a n d   t h e   p o t e n t i a l   o f   3 d   p r i n t i n g   a s   a n   e m e r g i n g   t e c h n o l o g y .', 'v e r y   i n t e r e s t i n g', 'T o p i c s   r e l a t e d   t o   t h e   e c o n o m y   o r   b u s i n e s s   a r e   n o t   s o m e t h i n g   I   a m   g o o d   a t   o r   e n j o y   r e a d i n g   a b o u t ,   t h i s   c o u r s e   h e l p e d   m e   l e a r n   a   b u n c h   o f   u s e f u l   t h i n g s   I   w o u l d   o t h e r w i s e   n o t   h a v e   p u t   e f f o r t   i n t o   s t u d y i n g .', 'I   w a s   a b l e   t o   u n d e r s t a n d   m a t e r i a l s   a n d   d e v i c e s   u s e d   i n   3 d   p r i n t i n g ,   s o f t w a r e   a s p e c t s   o f   3 d   p r i n t e r s   a n d   g e t   a n   i n t r o d u c t o r y   v i e w   o f   c u t t i n g   e d g e   a p p l i c a t i o n s   s u c h   a s   3 d   p r i n t i n g   i n   b i o m e d i c a l   a p p l i c a t i o n s', 'T h i s   c o u r s e   i s   v e r y   i n t e r e s t i n g ,   e n l i g h t e n i n g   a n d   a   w o n d e r f u l   i n t r o d u c t i o n   e s p e c i a l l y   f o r   l e a r n e r s   w i t h   n o   p r i o r   e x p e r i e n c e   w i t h   3 D   p r i n t i n g .', 'G r e a t   i n f o r m a t i o n   a n d   v i d e o s .   R e a d y   f o r   t h e   n e x t   s t e p . G u s', 'I f   y o u   a r e   i n t e r e s t e d   i n   h a v i n g   a   d e e p e r   i n s i g h t   o f   t h e   3 D   p r i n t i n g   t e c h n o l o g y ,   I   r e c o m m e n d   t h i s   c o u r s e .   T h e r e   a r e   n o   p r e r e q u i s i t e s ,   a l l   y o u   n e e d   i s   t h e   d e s i r e   t o   l e a r n .   T h i s   c o u r s e   w i l l   i n t r o d u c e   t o   t o   y o u   t h e   d i f f e r e n t   3 D   m a n u f a c t u r i n g   m e t h o d s ,   a n d   p r e s e n t   t o   y o u   t o   3 D   p r i n t i n g   c o m p a n i e s   t h a t   a r e   p r e s e n t l y   a c t i v e l y .   I f   y o u   w a n t   t o   t r a n s i t i o n s   i n t o   t h e   3 D   p r i n t i n g   f i e l d ,   I   b e l i e v e   t h i s   c o u r s e   i s   a   g o o d   s t a r t .', 'F u l l   I n d i v i d u a l s   s p e a k i n g', 'I t   s h o u l d   b e   l e s s   r e p e t i t i v e   a n d   f u l l   o f   u s e f u l   i n f o r m a t i o n s   a b o u t   a p p l i c a t i o n s .', 'V e r y   g o o d   i n s p i r a t i o n a l   e x a m p l e s .', 'G r e a t   c o u r s e .   I t   p r o v i d e d   a   g r e a t   d e g r e e   o f   k n o w l e d g e   a b o u t   a p p l i c a t i o n s   a n d   t h e   d e s i g n   p r o c e s s .', 'i t   c o u l d   b e   e a s i l y   c o m p r e s s e d   i n   a   t w o - w e e k s   c o u r s e . S o m e t i m e s   i n t e r v i e w s   a r e   t o o   l o n g   f o r   w h a t   t h e y   n e e d   t o   e x p l a i n .', 'g o o d   i n t r o d u c t i o n   i n t o   t h e   a p p l i c a t i o n s   o f   3 D   p r i n t i n g .', 'g o o d   o v e r v i e w   w h a t   y o u   c a n   d o   w i t h   t h i s   a m a z i n g   t e c h n o l o g y .', 'T h i s   s e s s i o n   w a s   t o o   l o n g   a n d   a   l i t t l e   b o r i n g .   C o u l d   b e   s h o r t e r   w i t h   t h e   r e l a t e d   c o n t e n t s .', 'e x e m p l a r y   e x a m p l e s   t o   h o w   3 d   P r i n t i n g   c a n   b e   a p p l i e d   t o   r e a l   w o r l d   p r o b l e m s .', 'G r e a t   c o n t e n t ,   I   s t i l l   n e e d   t o   g o   o v e r   i t   t o   g l e a n   t h e   w e a l t h   o f   i n f o r m a t i o n   t h a t   h a s   b e e n   p u t   t o g e t h e r   h e r e .', \"W h i l e   t h e r e   i s   s o m e   i n t e r e s t i n g   c o n t e n t ,   p a r t i c u a r l y   i n   t h e   l a s t   w e e k   o n   d e s i g n   t h i n k i n g ,   s o m e   o f   t h e   c o u r s e   t o p i c s   d o n ' t   s e e m   t o   h o l d   t o g e t h e r   w e l l ,   a n d   t h e   v i d e o s   i n c l u d e   a   l o t   o f   m o n o t o n o u s   t a l k i n g   h e a d s .   W h i l e   t h i s   c o n v e y s   t h e   b e n e f i t   o f   b e i n g   a b l e   t o   l i s t e n   t o   s o m e   o f   t h e   v i d e o s   w i t h o u t   c o n s i s t e n t l y   w a t c h i n g ,   t h e   i n t e r v i e w   q u e s t i o n s   a s k e d   o f   s p e a k e r s   a r e   s o m e t i m e s   p r i n t e d   o n   t h e   s c r e e n   a n d   n o t   a s k e d   v e r b a l l y .   I f   y o u ' r e   n o t   w a t c h i n g ,   y o u   m i s s   t h e   q u e s t i o n s .   I   w o u l d   l i k e   t o   s e e   t h i s   c o u r s e   r e f o c u s e d   o n   d e s i g n   t h i n k i n g   f o r   3 D   p r i n t i n g ,   w i t h   o t h e r   a p p l i c a t i o n s   i n t e r s p e r s e d   a s   e x a m p l e s   r a t h e r   t h a n   c o n s t i t u t i n g   t h e   l i o n ' s   s h a r e   o f   t h e   c o u r s e .   I   b e l i e v e   t h a t   f o r m a t   w o u l d   b o t h   b e   m o r e   e n g a g i n g   a n d   d o   a   b e t t e r   j o b   o f   d e l i v e r i n g   s k i l l s   w i t h   m o r e   o p p o r t u n i t y   f o r   p r a c t i c e .\", 'I   d o   n o t   f i n d   v e r y   i n t e r e s t i n g   t h i s   c o u r s e .   t o o   m a n y   i n t e r v i e w s .   I t   c o u l d   w o r k s   f o r   t h e   f i r s t   c o u r s e ,   b u t   n o t   f o r   t h e   s e c o n d .   I   w a s   e x p e c t i n g   t o   h a v e   m o r e   t e c h n i c a l   m a t e r i a l   a n d   l e s s o n s .', 'V e r y   g o o d   i n - d e p t h   l o o k   a t   t h e   p o t e n t i a l   a p p l i c a t i o n s   f o r   3 D   p r i n t i n g .   I   e s p e c i a l l y   l i k e d   t h e   s e c t i o n s   o n   d e v e l o p m e n t   a n d   e d u c a t i o n .   T h i s   c l a s s   g e t s   y o u   t h i n k i n g   a b o u t   t h e   p o t e n t i a l   r a m i f i c a t i o n s   o f   3 D   p r i n t i n g   a n d   h o w   i t   c a n   c h a n g e   t h e   e c o n o m y ,   b o t h   l o c a l l y   a n d   g l o b a l l y .', 'A   g o o d   c o u r s e   t o   s e e   t h e   v a r i o u s   m a r k e t   a n d   n o n - p r o f i t   a s p e c t s   o f   3 D   p r i n t i n g .   L o t   o f   e x a m p l e s   a n d   i n i t i a t i v e s ,   a', 'E x c e l l e n t   c o u r s e ,   c o u l d   d o   w i t h   a   l i t t l e   m o r e   v i s u a l   r e f e r e n c e s .   S o m e   o f   t h e   t a l k s   a r e   a   b i t   t o u g h   t o   g e t   t h r o u g h   s i n c e   y o u   a r e   w a t c h i n g   2   p e o p l e .   C o n t e n t   s t i l l   v e r y   v a l u a b l e .', 'A   v e r y   c o m p l e t e   c o u r s e ,   v e r y   i n t e r e s t i n g !', 'G r e a t   r e v i e w   o f   w h a t   c a n   b e   a c c o m p l i s h e d   t h r o u g h   3 D   p r i n t i n g .', 'G r e a t   c l a s s !', 'S u p e r !   I   d i d   n o t   e x p e c t   t h e   l e v e l   o f   e n g a g e m e n t   n e e d e d   t o   c o m p l e t e   t h e   c o u r s e   a n d   i t   w a s   a   v e r y   p l e a s a n t   s u r p r i s e ! B e i n g   a   l a y m a n   i n   t h e   3 D   p r i n t i n g   r e v o l u t i o n ,   t h i s   c o u r s e   r e a l l y   e x p a n d e d   m y   v i e w s   o n   3 D   p r i n t i n g .', 'A   l o t   o f   s p e a k i n g   w i t h o u t   a n y   s e n s e .   S k i p   i t   a t   a l l   c o s t', \"T h i s   i s   a   v e r y   b a s i c   c o u r s e   o n   3 D   P r i n t i n g   T e c h n o l o g y   b u t   e v e r y   a s p e c t   o f   t h e   t e c h n o l o g y   i s   a r t i s t i c a l l y   e x p l a i n e d .   A l l   t h e   v a r i o u s   p o i n t s   o f   v i e w   a r e   c o n s i d e r e d   a n d   a   3 6 0   d e g r e e   i d e a   i s   p a s s e d   o n   t o   t h e   s t u d e n t s . B y   t a k i n g   t h i s   c o u r s e ,   s t u d e n t s   w h o   a r e   c o n t e m p l a t i n g   a b o u t   t h e i r   i n v o l v e m e n t   i n   t h i s   r e v o l u t i o n a r y   t e c h n o l o g y   w i l l   b e   a b l e   t o   f i n a l l y   d e c i d e   t h e i r   f u t u r e   c o u r s e   o f   a c t i o n ,   w h i c h   I   b e l i e v e   w i l l   b e   p o s i t i v e . I n   s h o r t ,   t h i s   c o u r s e   w i l l   p r o v i d e   y o u   a   b a s e   w h e r e a s   t h e   f u r t h e r   c o u r s e s ,   I   t h i n k   ( a s   I   h a v e n ' t   t a k e n   u p   t h e   r e s t   o f   t h e   c o u r s e s   t i l l   n o w   b u t   I   i n t e n d   t o )   w i l l   a d d   s u f f i c i e n t   v a l u e   t o   y o u r   k n o w l e d g e . R e g a r d s .\", 'G r e a t   c o u r s e !   T h a n k   y o u !', 'V e r y   h e l p f u l   v i d e o s   t h a t   r e a l l y   t a k e s   y o u   i n t o   t h e   r e a l   w o r l d   o f   3 D   p r i n t i n g .   T h e   i n t e r v i e w s   w i t h   t h e   r e p r e s e n t a t i v e s   f r o m   v a r i o u s   i n d u s t r i e s   i n   t h i s   c o u r s e   h e l p e d   m e   u n d e r s t a n d   t h e   i m p o r t a n c e   o f   3 D   p r i n t i n g   /   a d d i t i v e   m a n u f a c t u r i n g   a s   a   c o n v e n i e n t   t o o l ,   a n d   a l s o   w h e r e   t o   l o o k   f o r   r e l a t e d   r e s o u r c e s .', 'G r e a t   i n t r o !   T h e   v i d e o s   w e r e   f a n t a s t i c   a n d   i n s t r u c t o r s   v e r y   e n g a g i n g .', 'E x c e l l e n t l y   f r a m e d   c o u r s e', 'I   l i k e   t h e   f a c t   t h a t   s o   m a n y   l e a d e r s   o f   3 D   p r i n t i n g   c a m e   f r o m   d i v e r s e   b a c k g r o u n d s .   A n d   t h e y   s h a r e   a n   i n f e c t i o u s   s p i r i t   o f   l e a r n i n g   a n d   p u s h i n g   t h e   b o u n d a r i e s   o f   s o f t w a r e ,   h a r d w a r e   a n d   m a t e r i a l s   s c i e n c e .', 'G r e a t   o v e r v i e w   a n d   i n t r o d u c t i o n   t o   3 D   p r i n t i n g', 'S o   g r e a t !   I f   y o u   w a n t   t o   o v e r v i e w   t h e   c u r r e n t   s i t u a t i o n   o f   3 D   p r i n t i n g   t e c h n o l o g y   a n d   d e v e l o p   s o   b u s i n e s s   o n   3 D   p r i n t i n g ,   y o u   h a v e   t o   w a t c h   t h i s', 'I   t h o u g h t   t h i s   w a s   a   g r e a t   h i g h - l e v e l   i n t r o   t o   t h e   w o r l d   o f   3 D   p r i n t i n g .', 'G o t   n e w   p r o s p e c t s   a n d   v i s i o n   o f   3 D   p r i n t i n g   t e c h', 'T h i s   w a s   a   g r e a t   c o u r s e ,   g r e a t   i m p l e m e n t a t i o n   a n d   m o t i v a t i o n   w a s   p r o v i d e d   w h i c h   h a s   d e f i n i t e l y   i n c r e a s e d   m y   e n t h u s i a s m   a n d   i   a m   r e a l l y   l o o k i n g   f o r w a r d   t o   o t h e r   c o u r s e s   t o   i n c r e a s e   m y   s k i l l s .', 'E x c e l l e n t   c o u r s e .   V e r y   w e l l   p r e s e n t e d .   I   e n j o y e d   n o t   o n l y   t h e   i n s t r u c t i o n   s e s s i o n s   b u t   a l s o   t h e   i n t e r v i e w s   w i t h   o t h e r   s p e c i a l i s t s   i n   t h i s   f a s c i n a t i n g   f i e l d .   A   g r e a t   c o u r s e   t o   i n s p i r e   o n e   t o   c o n t i n u e   p u r s u i n g   t h i s   a s   a   c a r e e r .', 'V e r y   i n f o r m a t i v e   a n   a p p r o a c h a b l e   c o u r s e .   I   r e a l l y   l i k e   t h e   w a y   A r i c   p r e s e n t s   t h e   i n f o r m a t i o n .   I   a m   e x c i t e d   t o   c o m p l e t e   t h e   r e s t   o f   t h e   S p e c i a l i z a t i o n .', 'g o o d   i n t r o d u c t i o n ! ! !', 'T h e   b e s t   c o u r s e   f o r   b e g i n n e r s .', 'T h a n k s   f o r   t h e   c o u r s e !   I t   h a s   o p e n e d   m e   t o   t h e   w o r l d   o f   3 D   p r i n t i n g   -   o n   w h e n   d i d   i t   s t a r t   a n d   w h e r e   i t   i s   n o w .   : )', 'T h i s   w a s   a   g r e a t   c o u r s e !   A r i c   R i n d f l e i s c h   e n c o u r a g e s   e n t h u s i a s m   o n   t h i s   n e w   t e c h o l o g y   r e v o l u t i o n .   I t   g a v e   m e   t h e   f u n d a m e n t a l   b a s i c s   o n   t h e   m a t t e r .   F u r t h e r m o r e   n o w   I   a m   m o r e   i n t e r e s t e d   o n   3 D   p r i n t i n g   t h a n   I   w a s   b e f o r e   s t a r t i n g   t h e   c o u r s e .   I   s u r e l y   f o l l o w   t h e   f o l l o w i n g   I l l i n o i s   U n i v e r s i t y   c o u r s e s   o n   3 D   P r i n t i n g !', 'G r e a t   i n t r o   t o   3 D   p r i n t i n g   a n d   t h e   m a n y ,   m a n y   w a y s   i t   c a n   b e   u s e d .', 'G o o d   e x p e r i e n c e   t o   s t a r t   w i t h', 'G i v e s   a   n i c e   o v e r   v i e w   a b o u t   w h a t   i s   3 d   p r i n t i n g .']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1118-ea0c8c319aa4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Review'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtf_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1650\u001b[0m         \"\"\"\n\u001b[0;32m   1651\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1652\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1653\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1654\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1056\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1058\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 989\u001b[1;33m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[0;32m    990\u001b[0m                                  \" contain stop words\")\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Создайте TF-IDF\n",
    "# Настройте векторизатор так, чтобы он использовал одно или 2 слова и количество слов и их сочетаний было 100,000\n",
    "\n",
    "tf_vect = TfidfVectorizer()\n",
    "r=[\" \".join(review) for review in df['Review'].values]\n",
    "print(r[0:100])\n",
    "x = tf_vect.fit_transform(r)  \n",
    "a=x.toarray()  \n",
    "tf_vect.get_feature_names()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторое представление в виде N-мернго вещественного вектора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images14/skip-gram.png\"><br>\n",
    "https://towardsdatascience.com/word-embedding-with-word2vec-and-fasttext-a209c1d3e12c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ’®¬ ў гбва®©бвўҐ C ­Ґ Ё¬ҐҐв ¬ҐвЄЁ.\n",
      " ‘ҐаЁ©­л© ­®¬Ґа в®¬ : 2269-C3D5\n",
      "\n",
      " ‘®¤Ґа¦Ё¬®Ґ Ї ЇЄЁ C:\\Users\\Ђ«ҐбЄ ­¤а\\Downloads\\images14\n",
      "\n",
      "06.12.2019  14:44    <DIR>          .\n",
      "06.12.2019  14:44    <DIR>          ..\n",
      "06.12.2019  14:44            69я920 Backpropagation-Through-Time-BPTT-in-standard-BRNNs.png\n",
      "06.12.2019  14:44            45я954 bag-of-words-matching.jpg\n",
      "06.12.2019  14:44            11я324 count_vec.jpeg\n",
      "06.12.2019  14:44           102я329 greff_lstm_diagram.png\n",
      "06.12.2019  14:44             6я184 ohe-words-sentense.png\n",
      "06.12.2019  14:44            80я336 plot_roc_0021.png\n",
      "06.12.2019  14:44            44я320 rnn.jpg\n",
      "06.12.2019  14:44           539я494 rnn-description.png\n",
      "06.12.2019  14:44           180я021 rnn-translation.png\n",
      "06.12.2019  14:44            56я215 skip-gram.png\n",
      "              10 д ©«®ў      1я136я097 Ў ©в\n",
      "               2 Ї Ї®Є  25я873я186я816 Ў ©в бў®Ў®¤­®\n"
     ]
    }
   ],
   "source": [
    "%ls images14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возможно, потребуется:\n",
    "\n",
    "# !pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1132-6ec7921f1f6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# define training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[1;34m'this'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'is'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'the'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'second'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sentence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# define training data\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "    ['this', 'is', 'the', 'second', 'sentence'],\n",
    "    ['yet', 'another', 'sentence'],\n",
    "    ['one', 'more', 'sentence'],\n",
    "    ['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "\n",
    "# access vector for one word\n",
    "print(model['sentence'])\n",
    "\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">ЗАДАНИЕ</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используйте следующий способ для создание вектора сообщения:\n",
    "* для каждого слова умножте его вектор на значение TF-IDF для этого слова\n",
    "* усредните эти вектора и примите этот вектор, как вектор сообщения\n",
    "* вычислите средний вектор по классу\n",
    "* определите класс сообщения из тестового набора по тому, к какому из векторов классов он ближе всего (используйте косинусное расстояние)\n",
    "\n",
    "Сделайте это на одном разбиении, полученнос с помощью test_train_split\n",
    "\n",
    "Пример реализации:\n",
    "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise\n",
    "# исползуйте cosine_similarity \n",
    "# Ваш код\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Рекуррентные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* В традиционной NN предполагается, что все входы и выходы независимы друг от друга\n",
    "* Есть задачи, в которых есть зависимость между предыдущими и последующими данными\n",
    "* Основная идея - получить возможность работать с последовательной информацией."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.youtube.com/watch?v=56TYLaQN4N8\">https://www.youtube.com/watch?v=56TYLaQN4N8</a> - Highly recommended!\n",
    "\n",
    "https://www.youtube.com/playlist?list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9 - Lecture from the course Neural Networks for Machine Learning, as taught by Geoffrey Hinton (University of Toronto) on Coursera in 2012. \n",
    "\n",
    "Link to the course: https://class.coursera.org/neuralnets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images14/rnn.jpg\">\n",
    "A recurrent neural network and the unfolding in time of the computation involved in its forward computation. Source: Nature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение RNN производится с помощью алгоритма _обратного распространения через время_ (Backpropagation Through Time - ___BPTT___))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images14/Backpropagation-Through-Time-BPTT-in-standard-BRNNs.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images14/greff_lstm_diagram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Машинный перевод __текстов__ делается с помощью RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images14/rnn-translation.png\">\n",
    "RNN for Machine Translation. Image Source: http://cs224d.stanford.edu/lectures/CS224d-Lecture8.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images14/rnn-description.png\">\n",
    "Deep Visual-Semantic Alignments for Generating Image Descriptions. Source: http://cs.stanford.edu/people/karpathy/deepimagesent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ’®¬ ў гбва®©бвўҐ C ­Ґ Ё¬ҐҐв ¬ҐвЄЁ.\n",
      " ‘ҐаЁ©­л© ­®¬Ґа в®¬ : 2269-C3D5\n",
      "\n",
      " ‘®¤Ґа¦Ё¬®Ґ Ї ЇЄЁ C:\\Users\\Ђ«ҐбЄ ­¤а\\Downloads\\images14\n",
      "\n",
      "06.12.2019  14:44    <DIR>          .\n",
      "06.12.2019  14:44    <DIR>          ..\n",
      "06.12.2019  14:44            69я920 Backpropagation-Through-Time-BPTT-in-standard-BRNNs.png\n",
      "06.12.2019  14:44            45я954 bag-of-words-matching.jpg\n",
      "06.12.2019  14:44            11я324 count_vec.jpeg\n",
      "06.12.2019  14:44           102я329 greff_lstm_diagram.png\n",
      "06.12.2019  14:44             6я184 ohe-words-sentense.png\n",
      "06.12.2019  14:44            80я336 plot_roc_0021.png\n",
      "06.12.2019  14:44            44я320 rnn.jpg\n",
      "06.12.2019  14:44           539я494 rnn-description.png\n",
      "06.12.2019  14:44           180я021 rnn-translation.png\n",
      "06.12.2019  14:44            56я215 skip-gram.png\n",
      "              10 д ©«®ў      1я136я097 Ў ©в\n",
      "               2 Ї Ї®Є  25я744я199я680 Ў ©в бў®Ў®¤­®\n"
     ]
    }
   ],
   "source": [
    "%ls images14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "# save np.load\n",
    "np_load_old = np.load\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "# call load_data with allow_pickle implicitly set to true\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "# restore np.load for future normal usage\n",
    "np.load = np_load_old\n",
    "\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Это хороший результат ???</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве домашнего задания:\n",
    "* разберитесь с ядром с kaggle \n",
    "    - https://www.kaggle.com/reiinakano/basic-nlp-bag-of-words-tf-idf-word2vec-lstm\n",
    "* попробуйте реализовать то же самое с Medium dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На следующее занятие:\n",
    "* внимательно повторите и почитайте про RNN\n",
    "* они потребуются нам для временных рядов (!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Prakashvanapalli/TensorFlow/blob/master/Blogposts/Backpropogation_with_Images.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Экзаменационные вопросы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Частотная модель текста\n",
    "* TF/IDF\n",
    "* Векторное представление слов и текстов\n",
    "* Рекуррентные нейронны сети"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
