{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://upload.wikimedia.org/wikipedia/commons/9/96/Pytorch_logo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом занятии помимо базового функционала `pytorch` будет использоваться библиотека для зрения **`torchvision`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch как конструктор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с датасетами\n",
    "\n",
    "Для загрузки данных pytorch опирается на такую сущность, как **`Dataset`**.\n",
    "\n",
    "Этот абстрактный класс определен в `torch.utils.data.dataset`:\n",
    "\n",
    "```python\n",
    "class Dataset(object):\n",
    "    \"\"\"An abstract class representing a Dataset.\n",
    "\n",
    "    All other datasets should subclass it. All subclasses should override\n",
    "    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n",
    "    supporting integer indexing in range from 0 to len(self) exclusive.\n",
    "    \"\"\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])\n",
    "```\n",
    "При определении нового источника данных мы создаем наследника данного класса и реализуем методы `__getitem__` и `__len__`.\n",
    "\n",
    "Пример готового такого класса — `torchvision.datasets.ImageFolder`, который позволяет создать датасет на основе директории с imagenet-подобной структурой поддиректорий (`./train/{class}` и `./val/{class}`):\n",
    "\n",
    "```python\n",
    "imagenet = torchvision.datasets.ImageFolder('path/to/imagenet_root/')\n",
    "```\n",
    "\n",
    "Кастомный пример — датасет, загружающий изображения на основе путей и классов из текстового файла:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "# default_loader - стандартная функция для загрузки изображений, использует accimage или PIL\n",
    "from torchvision.datasets.folder import default_loader\n",
    "\n",
    "class TxtList(Dataset):\n",
    "    def __init__(self, path, transform=None, loader=default_loader):\n",
    "        with open(path) as fin:\n",
    "            self.imgs = [s.strip().split() for s in fin.readlines()]\n",
    "\n",
    "        print(f'=> Found {len(self.imgs)} entries in {path}')\n",
    "\n",
    "        self.classes = sorted(set([_[1] for _ in self.imgs]))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.transform = transform\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.imgs[index]\n",
    "        target = self.class_to_idx[target]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo '/tmp/1.jpg\\tcat' > /tmp/dataset.tsv\n",
    "!echo '/tmp/2.jpg\\tcat' >> /tmp/dataset.tsv\n",
    "!echo '/tmp/3.jpg\\tdog' >> /tmp/dataset.tsv\n",
    "!echo '/tmp/4.jpg\\tcat' >> /tmp/dataset.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Found 4 entries in /tmp/dataset.tsv\n"
     ]
    }
   ],
   "source": [
    "catdog = TxtList('/tmp/dataset.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'dog']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catdog.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['/tmp/1.jpg', 'cat'],\n",
       " ['/tmp/2.jpg', 'cat'],\n",
       " ['/tmp/3.jpg', 'dog'],\n",
       " ['/tmp/4.jpg', 'cat']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catdog.imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(catdog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/1.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-09c24881b9b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# FileNotFoundError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcatdog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-625a7e4cd763>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/1.jpg'"
     ]
    }
   ],
   "source": [
    "# FileNotFoundError\n",
    "catdog[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torchvision` также содержит и другие готовые классы для стандартных датасетов: \n",
    "http://pytorch.org/docs/master/torchvision/datasets.html.\n",
    "\n",
    "Некоторые из них можно сразу и загрузить — например, **MNIST**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "![ -d '/tmp/mnist/' ] && rm -r '/tmp/mnist/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c977d7b4ab904dc0a17792d2a71b5743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/mnist/MNIST/raw/train-images-idx3-ubyte.gz to /tmp/mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /tmp/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bedc3a36c8d4bb9824601860de6eedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to /tmp/mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /tmp/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c44d4b90e7d4548b78594d538c2cf50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to /tmp/mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4511c64ee7d945bdabad423c34ed29aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/mnist/MNIST/raw\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serega/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:457: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "CPU times: user 1.85 s, sys: 476 ms, total: 2.33 s\n",
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "%time mnist = MNIST('/tmp/mnist/', train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "image, target = mnist[0]\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.array(image), 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#cc6666'>Внимание, задача!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Релизуйте ниже датасет **`UrlList`**, конструктор которого на вход принимает список ссылок на изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrlList(Dataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажите его работу на примере:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преобразования данных\n",
    "\n",
    "Конструктор примера, приведенного выше, также как и стандартного `ImageFolder`, принимают параметр `transform` (и `target_transform`).\n",
    "\n",
    "Они служат для того, чтобы загружаемые изображения (обычно это `PIL.Image`) или таргеты преобразовывать в тензоры нужного вида.\n",
    "\n",
    "В `torchvision` входит модуль `transforms` для стандартных примеров таких преобразований:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, `transforms.ToTensor()` преобразует `PIL`-изображение типа uint8 с доменом [0, 256) в тензор с доменом [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "to_tensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), tensor(0.), tensor(1.))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pil_image = mnist[0][0]\n",
    "th_image = to_tensor(pil_image)\n",
    "th_image.shape, th_image.min(), th_image.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А для того, чтобы нормализовывать изображения из ImageNet'а по стандартной схеме, можно объявить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для объединения нескольких преобразваний в одно есть `Compose`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "crop_and_tensorize = transforms.Compose([\n",
    "    transforms.CenterCrop(16),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(crop_and_tensorize(pil_image)[0].numpy(), 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При определении кастомного трансформера помимо конструктора нужно реализовать лишь метод `__call__`:\n",
    "\n",
    "```python\n",
    "class HorizontalFlip(object):\n",
    "    def __init__(self, mode=0):\n",
    "        self.method = mode\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be flipped.\n",
    "\n",
    "        Returns:\n",
    "            PIL.Image: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        if self.method:\n",
    "            return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        return img\n",
    "\n",
    "```\n",
    "\n",
    "С полным списком стандартных преобразований можно ознакомиться в http://pytorch.org/docs/master/torchvision/transforms.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#cc6666'>Внимание, задача!</font>\n",
    "\n",
    "Реализуйте класс-трансформер, осуществляющий с подаваемым на вход изображением случайное преобразование из группы диэдра $D_4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomD4(object):\n",
    "    def __call__(self, img):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажите его работу на примере из MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузчики данных\n",
    "\n",
    "Основная магия, для которой и нужны датасеты в приведенном выше виде, это загрузчики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузчики создаются на основе датасета и позволяют итерироваться по батчам из него.\n",
    "\n",
    "При этом батчи готовятся (загружаются картинки, обрабатываются и т.д.) сразу в нескольких фоновых процессах!\n",
    "\n",
    "Рассмотрим их на примере с MNIST, при этом добавим к нему трансформер, т.к. лоадеры работают с тензорами или скалярами, но не `PIL.Image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_mnist = MNIST('/tmp/mnist/', train=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_loader = DataLoader(transformed_mnist, batch_size=16, shuffle=True, num_workers=4)  # замечение про shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стандартные параметры приведены выше. При добавлении **`pin_memory=True`** батчи еще и сразу раскладываются по видеокартам.\n",
    "\n",
    "Посмотрим на работу в деле:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [00:06<00:00, 621.95it/s]\n"
     ]
    }
   ],
   "source": [
    "for images, targets in tqdm(mnist_loader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На выходе из лоадера получаем батчи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 28, 28])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество батчей заранее известно (чему был рад `tqdm`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3750"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример данных (при каждом повторении прохода по мнисту пример будет случайным):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0][0].numpy(), 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание моделей\n",
    "\n",
    "Для создания моделей наследуемся от `torch.nn.Module`, `torch.nn` также содержит стандартные \"кирпичики\" моделей.\n",
    "\n",
    "Функциональные версии кирпичей скрыты в `torch.nn.functional`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для макспулинга, активаций, апсемплинга и некоторых других операций есть как \"модульные\", так и \"функциональные\" версии:\n",
    "* `nn.MaxPool2d` / `F.max_pool2d`\n",
    "* `nn.ReLU` / `F.relu`\n",
    "* `nn.Upsample(mode='bilinar')` / `F.upsample(mode='bilinar')` — **deprecated** в пользу `F.interpolate`\n",
    "\n",
    "При использовании модульных версий слоев во время создания простой модели можно обходиться `nn.Sequential` для их \"склейки\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    nn.Conv2d(3, 64, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, 3, padding=1),\n",
    "    nn.ReLU()\n",
    "]\n",
    "unet_down1 = nn.Sequential(*layers)\n",
    "print(unet_down1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение моделей\n",
    "\n",
    "Для обучения моделей необходимо определить функцию потерь, их примеры содержатся все в том же модуле `torch.nn`:\n",
    "\n",
    "```python\n",
    "output = model(torch.cat(x, 1))\n",
    "target = torch.arange(1, 1001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Веса обновлять можно как вручную (или воспользоваться реализованным ранее оптимизатором):\n",
    "\n",
    "```python\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так и пользоваться стандартным оптимизатором из модуля `torch.optim`:\n",
    "```python\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для зануления градиентов теперь можно обращаться к оптимизатору:\n",
    "```python\n",
    "optimizer.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инференс, подсчет ошибки и бэкпроп делаем как раньше:\n",
    "```python\n",
    "output = net(x)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А для обновления весов используем оптимизатор:\n",
    "```python\n",
    "optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы с шагом обучения в **`optim`** есть подмодуль **`torch.optim.lr_scheduler`**:\n",
    "```python\n",
    "from torch.optim import lr_scheduler\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, обучение ResNet на ImageNet по стандартной схеме будет работать примерно так:\n",
    "```python\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "for epoch in range(100):\n",
    "     scheduler.step()  # == scheduler.step(epoch)\n",
    "     # train(...)\n",
    "     # validate(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch в бою"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание модели сегментации на примере U-Net\n",
    "\n",
    "![img](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто используемые свертки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, dilation=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один **блок кодировщика** состоит из двух последовательных сверток, активаций и опционального батчнорма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, batch_norm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.conv1 = conv3x3(in_channels, out_channels)\n",
    "        if self.batch_norm:\n",
    "            self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        if self.batch_norm:\n",
    "            self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderBlock(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = EncoderBlock(3, 64)\n",
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(4, 3, 128, 128)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(block(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Альтернативное объявление:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, batch_norm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential()\n",
    "        self.block.add_module('conv1', conv3x3(in_channels, out_channels))\n",
    "        if batch_norm:\n",
    "            self.block.add_module('bn1', nn.BatchNorm2d(out_channels))\n",
    "        self.block.add_module('relu1', nn.ReLU())\n",
    "        self.block.add_module('conv2', conv3x3(out_channels, out_channels))\n",
    "        if batch_norm:\n",
    "            self.block.add_module('bn2', nn.BatchNorm2d(out_channels))\n",
    "        self.block.add_module('relu2', nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderBlock(\n",
       "  (block): Sequential(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = EncoderBlock(3, 64)\n",
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(4, 3, 128, 128)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(block(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И \"функциональная версия\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(in_channels, out_channels, batch_norm=False):\n",
    "    block = nn.Sequential()\n",
    "    block.add_module('conv1', conv3x3(in_channels, out_channels))\n",
    "    if batch_norm:\n",
    "        block.add_module('bn1', nn.BatchNorm2d(out_channels))\n",
    "    block.add_module('relu1', nn.ReLU())\n",
    "    block.add_module('conv2', conv3x3(out_channels, out_channels))\n",
    "    if batch_norm:\n",
    "        block.add_module('bn2', nn.BatchNorm2d(out_channels))\n",
    "    block.add_module('relu2', nn.ReLU())\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = encoder_block(3, 64)\n",
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(4, 3, 128, 128)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(block(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кодировщик в целом состоит из рассмотренных блоков.\n",
    "\n",
    "Его конструкция определяется входными каналами, количеством фильтров в первом блоке и количеством блоков.\n",
    "\n",
    "Помним также, что для работы сети нам нужно запоминать промежуточные активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_filters, num_blocks):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_blocks = num_blocks\n",
    "        for i in range(num_blocks):\n",
    "            in_channels = in_channels if not i else num_filters * 2 ** (i - 1)\n",
    "            out_channels = num_filters * 2**i\n",
    "            self.add_module(f'block{i + 1}', encoder_block(in_channels, out_channels))\n",
    "            if i != num_blocks - 1:\n",
    "                self.add_module(f'pool{i + 1}', nn.MaxPool2d(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        acts = []\n",
    "        for i in range(self.num_blocks):\n",
    "            x = self.__getattr__(f'block{i + 1}')(x)\n",
    "            acts.append(x)\n",
    "            if i != self.num_blocks - 1:\n",
    "                x = self.__getattr__(f'pool{i + 1}')(x)\n",
    "        return acts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь как раз помогает подход к построению через с **`add_module`**, т.к. их количество переменно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (block1): Sequential(\n",
       "    (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block2): Sequential(\n",
       "    (conv1): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block3): Sequential(\n",
       "    (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block4): Sequential(\n",
       "    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(in_channels=3, num_filters=8, num_blocks=4)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([4, 8, 512, 512]),\n",
       " torch.Size([4, 16, 256, 256]),\n",
       " torch.Size([4, 32, 128, 128]),\n",
       " torch.Size([4, 64, 64, 64])]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(4, 3, 512, 512)\n",
    "\n",
    "[_.shape for _ in encoder(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Блок декодировщика состоит из апскейлинга входа \"снизу\", объединения двух входов и сверток как в кодировщике:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.uppool = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upconv = conv3x3(out_channels * 2, out_channels)\n",
    "        self.conv1 = conv3x3(out_channels * 2, out_channels)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, down, left):\n",
    "        x = self.uppool(down)\n",
    "        x = self.upconv(x)\n",
    "        x = torch.cat([left, x], 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = DecoderBlock(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 16, 256, 256]), torch.Size([4, 8, 512, 512]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1].shape, y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serega/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 512, 512])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block(y[1], y[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Декодировщик собираем из таких блоков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_filters, num_blocks):\n",
    "        super().__init__()\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            self.add_module(f'block{num_blocks - i}', DecoderBlock(num_filters * 2**i))\n",
    "\n",
    "    def forward(self, acts):\n",
    "        up = acts[-1]\n",
    "        for i, left in enumerate(acts[-2::-1]):\n",
    "            up = self.__getattr__(f'block{i + 1}')(up, left)\n",
    "        return up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(8, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 512, 512])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([4, 8, 512, 512]),\n",
       " torch.Size([4, 16, 256, 256]),\n",
       " torch.Size([4, 32, 128, 128]),\n",
       " torch.Size([4, 64, 64, 64])]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_.shape for _ in encoder(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nizhib/.anaconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/nizhib/.anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1961: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 512, 512])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(encoder(x)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net состоит из такого кодировщика и декодировщика, а также финального слоя классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=3, num_filters=64, num_blocks=4):\n",
    "        super().__init__()\n",
    "\n",
    "        print(f'=> Building {num_blocks}-blocks {num_filters}-filter U-Net')\n",
    "\n",
    "        self.encoder = Encoder(in_channels, num_filters, num_blocks)\n",
    "        self.decoder = Decoder(num_filters, num_blocks - 1)\n",
    "        self.final = nn.Conv2d(num_filters, num_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        acts = self.encoder(x)\n",
    "        x = self.decoder(acts)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Интеграционное тестирование\" (pytorch 0.3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building 4-blocks 64-filter U-Net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nizhib/.anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  import sys\n",
      "/home/nizhib/.anaconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/nizhib/.anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1961: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 416, 416])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "model = UNet(num_classes=1)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "images = Variable(torch.randn(4, 3, 416, 416), volatile=True)\n",
    "if torch.cuda.is_available():\n",
    "    images = images.cuda()\n",
    "\n",
    "model.forward(images).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же для **pytorch 0.4+**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nizhib/.anaconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/nizhib/.anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1961: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 416, 416])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "images = torch.randn(4, 3, 416, 416).to(device)\n",
    "\n",
    "model.forward(images).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На выходе получаем бинарную маску из линейных активаций.\n",
    "\n",
    "Для обучения такой модели используются функции потерь с **WithLogits** в названии.\n",
    "\n",
    "В вероятности их можно превращать с помощью `torch.sigmoid` (**0.4.1+**) (`torch.nn.functional.sigmoid` ранее)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование готового кодировщика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Структура блоков кодировщика рассмотренной только что сети сильно походит на таковую в сетях VGG:\n",
    "\n",
    "![img](https://www.pyimagesearch.com/wp-content/uploads/2017/03/imagenet_vgg16.png)\n",
    "\n",
    "Посмотрим на неё же из недр `torchvision`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG13 - версия сети с 2 сверткаим на каждый блок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg13()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): ReLU(inplace)\n",
       "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace)\n",
       "    (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): ReLU(inplace)\n",
       "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): ReLU(inplace)\n",
       "    (24): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификатор нам не нужен, интересуют только признаки.\n",
    "\n",
    "Они в свою очередь делятся на блоки conv-relu-conv-relu + maxpooling.\n",
    "\n",
    "Реализуем кодировщик на основе вычленения нужных блоков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG13Encoder(nn.Module):\n",
    "    def __init__(self, num_blocks, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        backbone = vgg13(pretrained=pretrained).features\n",
    "\n",
    "        self.num_blocks = num_blocks\n",
    "        for i in range(self.num_blocks):\n",
    "            block = nn.Sequential(*[backbone[j] for j in range(i * 5, i * 5 + 4)])\n",
    "            self.add_module(f'block{i + 1}', block)\n",
    "            if i != num_blocks - 1:\n",
    "                self.add_module(f'pool{i + 1}', nn.MaxPool2d(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        acts = []\n",
    "        for i in range(self.num_blocks):\n",
    "            x = self.__getattr__(f'block{i + 1}')(x)\n",
    "            acts.append(x)\n",
    "            if i != self.num_blocks - 1:\n",
    "                x = self.__getattr__(f'pool{i + 1}')(x)\n",
    "        return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG13Encoder(\n",
       "  (block1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "  )\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_encoder = VGG13Encoder(num_blocks=4)\n",
    "vgg_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним с \"ванильным\" кодировщиком:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (block1): Sequential(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block2): Sequential(\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block3): Sequential(\n",
       "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block4): Sequential(\n",
       "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(in_channels=3, num_filters=64, num_blocks=4)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили идентичную структуру!\n",
    "\n",
    "Только теперь у нас уже есть предобученные слои для выделения признаков в кодировщике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#cc6666'>Внимание, задача!</font>\n",
    "\n",
    "\n",
    "**Реализуйте датасет** для игрушечной задачи сегментации, генерирующий такие данные:\n",
    "\n",
    "![img](https://raw.githubusercontent.com/jakeret/tf_unet/master/docs/toy_problem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.е. необходимо генерировать цветные эллипсы на цветном фоне и к итоговой картинке добавлять шум разной природы.\n",
    "\n",
    "При этом датасет выдает как изображение, так и его бинарную маску."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "def get_random_ellips(img, target, sz):\n",
    "    center = (np.random.randint(sz[0]), np.random.randint(sz[1]))\n",
    "    axes = (np.random.randint(20, sz[0] / 3), np.random.randint(20, sz[0] / 3))\n",
    "    angle = np.random.randint(180)\n",
    "    color = tuple(map(lambda x: int(x), (np.random.randint(255, size=3))))\n",
    "    return cv2.ellipse(img,center,axes,angle,0,360,color,-1), \\\n",
    "            cv2.ellipse(target,center,axes,angle,0,360, (0,0,0),-1)\n",
    "            \n",
    "\n",
    "class Ellipses(Dataset):\n",
    "    def __init__(self, size_, length, max_num = 7, transform=None):\n",
    "        super().__init__()\n",
    "        self.size_ = size_\n",
    "        self.length = length\n",
    "        self.transform = transform\n",
    "        self.max_num = max_num\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        #img = (np.random.rand(*self.size_)*255).astype('uint8')\n",
    "        img = np.full((self.size_), \\\n",
    "                        tuple(map(lambda x: int(x), (np.random.randint(255, size=3)))), dtype=np.uint8)\n",
    "        target = np.ones((self.size_[0], self.size_[1]), dtype=np.uint8)*255\n",
    "        # generate ellipses\n",
    "        for i in range(int(np.random.randint(2, self.max_num))):\n",
    "            img, target = get_random_ellips(img, target, self.size_)\n",
    "        \n",
    "        # add noizes\n",
    "        gaussian_noise = np.zeros((self.size_),dtype=np.uint8)\n",
    "        cv2.randn(gaussian_noise, 128, 20)\n",
    "        uniform_noise = np.zeros((self.size_),dtype=np.uint8)\n",
    "        cv2.randu(uniform_noise,0,255)\n",
    "        gaussian_noise = (gaussian_noise*np.random.rand(1)*2).astype(np.uint8)\n",
    "        uniform_noise = (uniform_noise*np.random.rand(1)*2).astype(np.uint8)\n",
    "        img = cv2.add(img, gaussian_noise)\n",
    "        img = cv2.add(img, uniform_noise)\n",
    "        \n",
    "        img, target = transforms.ToTensor()(img), transforms.ToTensor()(target)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            target = self.transform(target)\n",
    "        target = target.type(torch.LongTensor).squeeze(0)\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "        \n",
    "# trainset = Ellipses([512,512,3], 5)       \n",
    "# background, target = trainset[0]\n",
    "# print(background.shape)\n",
    "# print(target.shape)\n",
    "\n",
    "############################################################3\n",
    "# sz = [512, 512, 3]       \n",
    "# background = (np.random.rand(*sz)*255).astype('uint8')\n",
    "# target = np.ones((512, 512), dtype=np.uint8)*255\n",
    "\n",
    "# center = (255,255)\n",
    "# axes = (100,50)\n",
    "# angle = 30\n",
    "# color = (98, 121, 13)\n",
    "\n",
    "# target = cv2.ellipse(target,center,axes,angle,0,360, (0,0,0),-1)\n",
    "# background = cv2.ellipse(background,center,axes,angle,0,360,color,-1)\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "# ax1.imshow(background)\n",
    "# ax2.imshow(target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим вспомогательную функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "def moy_variant(surname):\n",
    "    return int(hashlib.md5(surname.encode().lower()).hexdigest()[-1], 16) % 2\n",
    "print(moy_variant('Сахаров'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения этой задачи **реализуйте сеть EDANet** (https://arxiv.org/abs/1809.06323, если `moy_variant` от вашей фамилии на русском языке в именительном падеже выдает 1) **или BiSeNet** (https://arxiv.org/abs/1808.00897, иначе) на основе кодировщика двух предобученных сетей `torchvision` из семейств **ResNet и DenseNet**. В качестве лосса рекомендуется использовать `BCEWithLogitsLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "class resnet50(torch.nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.features = models.resnet50(pretrained=pretrained)\n",
    "        self.conv1 = self.features.conv1\n",
    "        self.bn1 = self.features.bn1\n",
    "        self.relu = self.features.relu\n",
    "        self.maxpool1 = self.features.maxpool\n",
    "        self.layer1 = self.features.layer1\n",
    "        self.layer2 = self.features.layer2\n",
    "        self.layer3 = self.features.layer3\n",
    "        self.layer4 = self.features.layer4\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.relu(self.bn1(x))\n",
    "        x = self.maxpool1(x)\n",
    "        feature1 = self.layer1(x)  # 1 / 4\n",
    "        feature2 = self.layer2(feature1)  # 1 / 8\n",
    "        feature3 = self.layer3(feature2)  # 1 / 16\n",
    "        feature4 = self.layer4(feature3)  # 1 / 32\n",
    "        tail = self.pool(feature4)\n",
    "        return feature3, feature4, tail\n",
    "    \n",
    "class densenet121(torch.nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.features = models.densenet121(pretrained=pretrained).features\n",
    "        self.conv0 = self.features.conv0\n",
    "        self.norm0 = self.features.norm0\n",
    "        self.relu0 = self.features.relu0\n",
    "        self.maxpool0 = self.features.pool0\n",
    "        self.denseblock1 = self.features.denseblock1\n",
    "        self.transition1 = self.features.transition1\n",
    "        self.denseblock2 = self.features.denseblock2\n",
    "        self.transition2 = self.features.transition2\n",
    "        self.denseblock3 = self.features.denseblock3\n",
    "        self.transition3 = self.features.transition3\n",
    "        self.denseblock4 = self.features.denseblock4\n",
    "        self.norm5 = self.features.norm5\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv0(input)\n",
    "        x = self.relu0(self.norm0(x))\n",
    "        x = self.maxpool0(x)\n",
    "        x = self.transition1(self.denseblock1(x))\n",
    "        x = self.transition2(self.denseblock2(x))\n",
    "        f = self.denseblock3(x) # 1 / 16\n",
    "        feature3 = self.transition3(f)\n",
    "        feature4 = self.norm5(self.denseblock4(feature3)) # 1 / 32\n",
    "        tail = self.pool(feature4)\n",
    "        return f, feature4, tail\n",
    "\n",
    "class ConvBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2,padding=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        return self.relu(self.bn(x))\n",
    "\n",
    "class Spatial_path(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convblock1 = ConvBlock(in_channels=3, out_channels=64)\n",
    "        self.convblock2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.convblock3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.convblock1(input)\n",
    "        x = self.convblock2(x)\n",
    "        x = self.convblock3(x)\n",
    "        return x\n",
    "\n",
    "class AttentionRefinementModule(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.in_channels = in_channels\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # global average pooling\n",
    "        x = self.avgpool(input)\n",
    "        assert self.in_channels == x.size(1), 'in_channels and out_channels should all be {}'.format(x.size(1))\n",
    "        x = self.conv(x)\n",
    "        # x = self.sigmoid(self.bn(x))\n",
    "        x = self.sigmoid(x)\n",
    "        # channels of input and x should be same\n",
    "        x = torch.mul(input, x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeatureFusionModule(torch.nn.Module):\n",
    "    def __init__(self, num_classes, in_channels):\n",
    "        super().__init__()\n",
    "        # self.in_channels = input_1.channels + input_2.channels\n",
    "        # resnet50 3328 = 256(from context path) + 1024(from spatial path) + 2048(from spatial path)\n",
    "        # densenet121  2304 = 256(from context path) + 1024(from spatial path) + 1024(from spatial path)\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.convblock = ConvBlock(in_channels=self.in_channels, out_channels=num_classes, stride=1)\n",
    "        self.conv1 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "\n",
    "\n",
    "    def forward(self, input_1, input_2):\n",
    "        x = torch.cat((input_1, input_2), dim=1)\n",
    "        assert self.in_channels == x.size(1), 'in_channels of ConvBlock should be {}'.format(x.size(1))\n",
    "        feature = self.convblock(x)\n",
    "        x = self.avgpool(feature)\n",
    "\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.sigmoid(self.conv2(x))\n",
    "        x = torch.mul(feature, x)\n",
    "        x = torch.add(x, feature)\n",
    "        return x\n",
    "\n",
    "class BiSeNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes, context_path='resnet50'):\n",
    "        super().__init__()\n",
    "        # build spatial path\n",
    "        self.saptial_path = Spatial_path()\n",
    "\n",
    "        if context_path == 'resnet50':\n",
    "            # build context path\n",
    "            self.context_path = resnet50()\n",
    "\n",
    "            # build attention refinement module  for resnet 50\n",
    "            #if context_path == 'resnet50':\n",
    "            self.attention_module1 = AttentionRefinementModule(1024, 1024)\n",
    "            self.attention_module2 = AttentionRefinementModule(2048, 2048)\n",
    "            # supervision block\n",
    "            self.supervision1 = nn.Conv2d(in_channels=1024, out_channels=num_classes, kernel_size=1)\n",
    "            self.supervision2 = nn.Conv2d(in_channels=2048, out_channels=num_classes, kernel_size=1)\n",
    "            # build feature fusion module\n",
    "            self.feature_fusion_module = FeatureFusionModule(num_classes, 3328)\n",
    "\n",
    "        elif context_path == 'densenet121':\n",
    "            # build context path\n",
    "            self.context_path = densenet121()\n",
    "            \n",
    "            # build attention refinement module for densenet121\n",
    "            self.attention_module1 = AttentionRefinementModule(1024, 1024)\n",
    "            self.attention_module2 = AttentionRefinementModule(1024, 1024)\n",
    "            # supervision block\n",
    "            self.supervision1 = nn.Conv2d(in_channels=1024, out_channels=num_classes, kernel_size=1)\n",
    "            self.supervision2 = nn.Conv2d(in_channels=1024, out_channels=num_classes, kernel_size=1)\n",
    "            # build feature fusion module\n",
    "            self.feature_fusion_module = FeatureFusionModule(num_classes, 2304)\n",
    "        else:\n",
    "            print('Error: unspport context_path network \\n')\n",
    "\n",
    "        # build final convolution\n",
    "        self.conv = nn.Conv2d(in_channels=num_classes, out_channels=num_classes, kernel_size=1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "   \n",
    "\n",
    "    def forward(self, input):\n",
    "        # output of spatial path\n",
    "        sx = self.saptial_path(input)\n",
    "\n",
    "        # output of context path\n",
    "        cx1, cx2, tail = self.context_path(input)\n",
    "        cx1 = self.attention_module1(cx1)\n",
    "        cx2 = self.attention_module2(cx2)\n",
    "        cx2 = torch.mul(cx2, tail)\n",
    "        # upsampling\n",
    "        cx1 = torch.nn.functional.interpolate(cx1, size=sx.size()[-2:], mode='bilinear')\n",
    "        cx2 = torch.nn.functional.interpolate(cx2, size=sx.size()[-2:], mode='bilinear')\n",
    "        cx = torch.cat((cx1, cx2), dim=1)\n",
    "\n",
    "        if self.training == True:\n",
    "            cx1_sup = self.supervision1(cx1)\n",
    "            cx2_sup = self.supervision2(cx2)\n",
    "            cx1_sup = torch.nn.functional.interpolate(cx1_sup, size=input.size()[-2:], mode='bilinear')\n",
    "            cx2_sup = torch.nn.functional.interpolate(cx2_sup, size=input.size()[-2:], mode='bilinear')\n",
    "\n",
    "        # output of feature fusion module\n",
    "        result = self.feature_fusion_module(sx, cx)\n",
    "\n",
    "        # upsampling\n",
    "        result = torch.nn.functional.interpolate(result, scale_factor=8, mode='bilinear')\n",
    "        result = self.conv(result)\n",
    "        result = self.softmax(result)\n",
    "\n",
    "        if self.training == True:\n",
    "            cx1_sup = self.softmax(cx1_sup)\n",
    "            cx2_sup = self.softmax(cx2_sup)\n",
    "            return result, cx1_sup, cx2_sup\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "def nanmean(x):\n",
    "    \"\"\"Computes the arithmetic mean ignoring any NaNs.\"\"\"\n",
    "    return torch.mean(x[x == x])\n",
    "\n",
    "EPS = 1e-10\n",
    "\n",
    "def _fast_hist(true, pred, num_classes):\n",
    "    mask = (true >= 0) & (true < num_classes)\n",
    "    k = num_classes * true[mask] + pred[mask]\n",
    "    #k[k<0] = 0\n",
    "    hist = torch.bincount(\n",
    "        k,\n",
    "        minlength=num_classes ** 2,\n",
    "    ).reshape(num_classes, num_classes).float()\n",
    "    return hist\n",
    "\n",
    "def overall_pixel_accuracy(hist):\n",
    "    correct = torch.diag(hist).sum()\n",
    "    total = hist.sum()\n",
    "    overall_acc = correct / (total + EPS)\n",
    "    return overall_acc\n",
    "\n",
    "def eval_metric(targ, pred, num_classes):\n",
    "    '''\n",
    "    targ [b, 1, h, w] or [b, h, w]\n",
    "    pred [b, h, w] (after argmax(dim=1))\n",
    "    '''\n",
    "    #num_classes = 2\n",
    "    hist = torch.zeros((num_classes, num_classes)).to(targ.device)\n",
    "    for t, p in zip(targ, pred):\n",
    "        t = t.type(torch.int64)\n",
    "        p = p.type(torch.int64)\n",
    "        hist += _fast_hist(t.flatten(), p.flatten(), num_classes)\n",
    "    overall_acc = overall_pixel_accuracy(hist)\n",
    "    return overall_acc\n",
    "\n",
    "\n",
    "def val(num_classes, model, dataloader):\n",
    "    print('start val!')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_accuracy = []\n",
    "        for i, (data, label) in enumerate(dataloader):\n",
    "            data = data.cuda()\n",
    "            label = label.cuda()\n",
    "            pred = model(data)\n",
    "            accuracy = eval_metric(label, torch.argmax(pred, dim=1), num_classes)\n",
    "            all_accuracy.append(accuracy)\n",
    "        all_accuracy = torch.tensor(all_accuracy).to(data.device)\n",
    "        miou = torch.mean(all_accuracy)\n",
    "    print('mIoU for validation: %.3f' % float(miou))\n",
    "    return float(miou)\n",
    "\n",
    "def poly_lr_scheduler(optimizer, init_lr, iter, lr_decay_iter=1,\n",
    "                      max_iter=100, power=0.9):\n",
    "\tlr = init_lr*(1 - iter/max_iter)**power\n",
    "\toptimizer.param_groups[0]['lr'] = lr\n",
    "\treturn lr  \n",
    "    \n",
    "def train(model, optimizer, dataloader_train, dataloader_val):\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    max_miou = 0\n",
    "    step = 0\n",
    "    writer = SummaryWriter('board')\n",
    "    for epoch in range(100):\n",
    "        print('epoch:', epoch)\n",
    "        lr = poly_lr_scheduler(optimizer, 0.002, iter=epoch, max_iter=100)\n",
    "        model.train()\n",
    "        loss_record = []\n",
    "        for i, (data, label) in enumerate(dataloader_train):\n",
    "            data = data.cuda()\n",
    "            label = label.cuda()\n",
    "            output, output_sup1, output_sup2 = model(data)\n",
    "            loss1 = loss_func(output, label)\n",
    "            loss2 = loss_func(output_sup1, label)\n",
    "            loss3 = loss_func(output_sup2, label)\n",
    "            loss = loss1 + loss2 + loss3\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            step += 1\n",
    "            writer.add_scalar('loss_step', loss, step)\n",
    "            loss_record.append(loss.item())\n",
    "        loss_train_mean = np.mean(loss_record)\n",
    "        writer.add_scalar('epoch/loss_epoch_train', float(loss_train_mean), epoch)\n",
    "        print('loss for train : %f' % (loss_train_mean))\n",
    "        if epoch % 1 == 0:\n",
    "            if not os.path.isdir('./checkpoint'):\n",
    "                os.mkdir('./checkpoint')\n",
    "            torch.save(model.module.state_dict(),\n",
    "                       os.path.join('./checkpoint', (str(epoch)+'-' +'BiSeNet.pth')))\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            miou = val(2, model, dataloader_val)\n",
    "            if miou > max_miou:\n",
    "                max_miou = miou\n",
    "                torch.save(model.module.state_dict(),\n",
    "                           os.path.join('./checkpoint', 'best_model.pth'))\n",
    "            writer.add_scalar('epoch/miou val', miou, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"8,9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "loss for train : 1.039430\n",
      "start val!\n",
      "mIoU for validation: 0.984\n",
      "epoch: 1\n",
      "loss for train : 0.972672\n",
      "start val!\n",
      "mIoU for validation: 0.991\n",
      "epoch: 2\n",
      "loss for train : 0.962392\n",
      "start val!\n",
      "mIoU for validation: 0.985\n",
      "epoch: 3\n",
      "loss for train : 0.964905\n",
      "start val!\n",
      "mIoU for validation: 0.986\n",
      "epoch: 4\n",
      "loss for train : 0.957192\n",
      "start val!\n",
      "mIoU for validation: 0.993\n",
      "epoch: 5\n",
      "loss for train : 0.957188\n",
      "start val!\n",
      "mIoU for validation: 0.992\n",
      "epoch: 6\n",
      "loss for train : 0.950825\n",
      "start val!\n",
      "mIoU for validation: 0.997\n",
      "epoch: 7\n",
      "loss for train : 0.948451\n",
      "start val!\n",
      "mIoU for validation: 0.992\n",
      "epoch: 8\n",
      "loss for train : 0.953692\n",
      "start val!\n",
      "mIoU for validation: 0.996\n",
      "epoch: 9\n",
      "loss for train : 0.951340\n",
      "start val!\n",
      "mIoU for validation: 0.993\n",
      "epoch: 10\n",
      "loss for train : 0.953223\n",
      "start val!\n",
      "mIoU for validation: 0.997\n",
      "epoch: 11\n",
      "loss for train : 0.950465\n",
      "start val!\n",
      "mIoU for validation: 0.995\n",
      "epoch: 12\n",
      "loss for train : 0.947897\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 13\n",
      "loss for train : 0.946625\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 14\n",
      "loss for train : 0.948803\n",
      "start val!\n",
      "mIoU for validation: 0.990\n",
      "epoch: 15\n",
      "loss for train : 0.949278\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 16\n",
      "loss for train : 0.947931\n",
      "start val!\n",
      "mIoU for validation: 0.997\n",
      "epoch: 17\n",
      "loss for train : 0.948642\n",
      "start val!\n",
      "mIoU for validation: 0.997\n",
      "epoch: 18\n",
      "loss for train : 0.946889\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 19\n",
      "loss for train : 0.946482\n",
      "start val!\n",
      "mIoU for validation: 0.997\n",
      "epoch: 20\n",
      "loss for train : 0.948090\n",
      "start val!\n",
      "mIoU for validation: 0.997\n",
      "epoch: 21\n",
      "loss for train : 0.945862\n",
      "start val!\n",
      "mIoU for validation: 0.999\n",
      "epoch: 22\n",
      "loss for train : 0.945092\n",
      "start val!\n",
      "mIoU for validation: 0.999\n",
      "epoch: 23\n",
      "loss for train : 0.944713\n",
      "start val!\n",
      "mIoU for validation: 0.999\n",
      "epoch: 24\n",
      "loss for train : 0.944513\n",
      "start val!\n",
      "mIoU for validation: 0.997\n",
      "epoch: 25\n",
      "loss for train : 0.947363\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 26\n",
      "loss for train : 0.947770\n",
      "start val!\n",
      "mIoU for validation: 0.999\n",
      "epoch: 27\n",
      "loss for train : 0.945676\n",
      "start val!\n",
      "mIoU for validation: 0.999\n",
      "epoch: 28\n",
      "loss for train : 0.944889\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 29\n",
      "loss for train : 0.944875\n",
      "start val!\n",
      "mIoU for validation: 0.999\n",
      "epoch: 30\n",
      "loss for train : 0.944500\n",
      "start val!\n",
      "mIoU for validation: 0.999\n",
      "epoch: 31\n",
      "loss for train : 0.944242\n",
      "start val!\n",
      "mIoU for validation: 0.999\n",
      "epoch: 32\n",
      "loss for train : 0.944887\n",
      "start val!\n",
      "mIoU for validation: 0.992\n",
      "epoch: 33\n",
      "loss for train : 0.947633\n",
      "start val!\n",
      "mIoU for validation: 0.999\n",
      "epoch: 34\n",
      "loss for train : 0.944739\n",
      "start val!\n",
      "mIoU for validation: 0.999\n",
      "epoch: 35\n",
      "loss for train : 0.944282\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 36\n",
      "loss for train : 0.944089\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 37\n",
      "loss for train : 0.943955\n",
      "start val!\n",
      "mIoU for validation: 0.997\n",
      "epoch: 38\n",
      "loss for train : 0.943876\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 39\n",
      "loss for train : 0.943851\n",
      "start val!\n",
      "mIoU for validation: 0.995\n",
      "epoch: 40\n",
      "loss for train : 0.943968\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 41\n",
      "loss for train : 0.945588\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 42\n",
      "loss for train : 0.944382\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 43\n",
      "loss for train : 0.943883\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 44\n",
      "loss for train : 0.943771\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 45\n",
      "loss for train : 0.943759\n",
      "start val!\n",
      "mIoU for validation: 0.999\n",
      "epoch: 46\n",
      "loss for train : 0.943670\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 47\n",
      "loss for train : 0.943616\n",
      "start val!\n",
      "mIoU for validation: 0.999\n",
      "epoch: 48\n",
      "loss for train : 0.943591\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 49\n",
      "loss for train : 0.943541\n",
      "start val!\n",
      "mIoU for validation: 0.999\n",
      "epoch: 50\n",
      "loss for train : 0.944073\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 51\n",
      "loss for train : 0.945178\n",
      "start val!\n",
      "mIoU for validation: 0.999\n",
      "epoch: 52\n",
      "loss for train : 0.943974\n",
      "start val!\n",
      "mIoU for validation: 0.993\n",
      "epoch: 53\n",
      "loss for train : 0.943699\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 54\n",
      "loss for train : 0.943558\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 55\n",
      "loss for train : 0.943499\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 56\n",
      "loss for train : 0.943465\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 57\n",
      "loss for train : 0.943429\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 58\n",
      "loss for train : 0.943408\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 59\n",
      "loss for train : 0.943384\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 60\n",
      "loss for train : 0.943376\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 61\n",
      "loss for train : 0.943345\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 62\n",
      "loss for train : 0.943322\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 63\n",
      "loss for train : 0.943297\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 64\n",
      "loss for train : 0.943287\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 65\n",
      "loss for train : 0.943279\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 66\n",
      "loss for train : 0.943264\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 67\n",
      "loss for train : 0.943233\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 68\n",
      "loss for train : 0.943208\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 69\n",
      "loss for train : 0.943188\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 70\n",
      "loss for train : 0.943173\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 71\n",
      "loss for train : 0.943157\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 72\n",
      "loss for train : 0.943150\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 73\n",
      "loss for train : 0.943146\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 74\n",
      "loss for train : 0.943144\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 75\n",
      "loss for train : 0.943129\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 76\n",
      "loss for train : 0.943121\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 77\n",
      "loss for train : 0.943106\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 78\n",
      "loss for train : 0.943078\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 79\n",
      "loss for train : 0.943079\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 80\n",
      "loss for train : 0.943087\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 81\n",
      "loss for train : 0.943032\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 82\n",
      "loss for train : 0.943011\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 83\n",
      "loss for train : 0.942999\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 84\n",
      "loss for train : 0.942993\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 85\n",
      "loss for train : 0.942992\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 86\n",
      "loss for train : 0.942993\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 87\n",
      "loss for train : 0.942975\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 88\n",
      "loss for train : 0.942960\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 89\n",
      "loss for train : 0.942960\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 90\n",
      "loss for train : 0.942975\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 91\n",
      "loss for train : 0.942993\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 92\n",
      "loss for train : 0.942971\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 93\n",
      "loss for train : 0.942935\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 94\n",
      "loss for train : 0.942901\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 95\n",
      "loss for train : 0.942883\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 96\n",
      "loss for train : 0.942871\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 97\n",
      "loss for train : 0.942865\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 98\n",
      "loss for train : 0.942857\n",
      "start val!\n",
      "mIoU for validation: 0.998\n",
      "epoch: 99\n",
      "loss for train : 0.942849\n",
      "start val!\n",
      "mIoU for validation: 0.998\n"
     ]
    }
   ],
   "source": [
    "# net = BiSeNet(2, context_path='resnet50')\n",
    "net = BiSeNet(2, context_path='densenet121')\n",
    "net = net.cuda()\n",
    "net = torch.nn.DataParallel(net)\n",
    "trainset = Ellipses([512,512,3], 10000)\n",
    "valset = Ellipses([512,512,3], 1000)\n",
    "train_loader = DataLoader(trainset, batch_size=16, num_workers=8,\n",
    "                              pin_memory=True)\n",
    "val_loader = DataLoader(valset, batch_size=16, num_workers=8,\n",
    "                              pin_memory=True)\n",
    "optimizer = torch.optim.Adam(net.parameters(), 0.002)\n",
    "train(net, optimizer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet50 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f50b560acf8>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#results of resnet50 backbone\n",
    "from torch.utils.data import DataLoader\n",
    "valset = Ellipses([512,512,3], 10000)\n",
    "\n",
    "num_classes = 2\n",
    "img, targ = valset[0]\n",
    "img = img.unsqueeze(0)\n",
    "targ = targ.unsqueeze(0)\n",
    "img, targ = img.cuda(), targ.cuda()\n",
    "\n",
    "net = BiSeNet(2)\n",
    "net = net.cuda()\n",
    "net.load_state_dict(torch.load('./checkpoint/best_model_resnet50.pth'))\n",
    "net.eval()\n",
    "pred = net(img)\n",
    "pred = torch.argmax(pred, dim=1)\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(1, 3)\n",
    "ax0.imshow(img.squeeze().permute(1,2,0).cpu().detach().numpy())\n",
    "ax1.imshow(pred.squeeze().cpu().detach().numpy())\n",
    "ax2.imshow(targ.squeeze().cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start val!\n",
      "mIoU for validation: 0.999\n"
     ]
    }
   ],
   "source": [
    "# resnet50 version\n",
    "fin = val(2, net, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](resnet50.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet121 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f50b2983470>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#results of densenet121 backbone\n",
    "from torch.utils.data import DataLoader\n",
    "valset = Ellipses([512,512,3], 10000)\n",
    "\n",
    "num_classes = 2\n",
    "img, targ = valset[0]\n",
    "img = img.unsqueeze(0)\n",
    "targ = targ.unsqueeze(0)\n",
    "img, targ = img.cuda(), targ.cuda()\n",
    "\n",
    "net = BiSeNet(2, context_path='densenet121')\n",
    "net = net.cuda()\n",
    "net.load_state_dict(torch.load('./checkpoint/best_model_densenet121.pth'))\n",
    "net.eval()\n",
    "pred = net(img)\n",
    "pred = torch.argmax(pred, dim=1)\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(1, 3)\n",
    "ax0.imshow(img.squeeze().permute(1,2,0).cpu().detach().numpy())\n",
    "ax1.imshow(pred.squeeze().cpu().detach().numpy())\n",
    "ax2.imshow(targ.squeeze().cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start val!\n",
      "mIoU for validation: 0.999\n"
     ]
    }
   ],
   "source": [
    "# resnet50 version\n",
    "fin = val(2, net, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](densenet121.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проиллюстрируйте процесс обучения** сети скриншотами из `tensorboard` (см. `lanpa/tensorboardX`).\n",
    "\n",
    "Необходимо в частности отобразить кривую обучения и примеры эволюции качества выдаваемых масок."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
